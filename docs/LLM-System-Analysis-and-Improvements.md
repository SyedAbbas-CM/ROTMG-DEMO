# LLM Boss System: Current State Analysis & Improvement Plan

**Date:** October 24, 2025
**Status:** Analysis Complete

---

## Executive Summary

The ROTMG-DEMO LLM boss system is a **sophisticated but incomplete** AI-driven game boss architecture. The foundation is excellent, but only **22% of planned capabilities** are implemented. This document outlines current limitations and proposes comprehensive improvements.

---

## ðŸ” Current System Architecture

### Components (All Working âœ…)

1. **BossManager** - Boss state management
2. **LLMBossController** - AI decision coordination
3. **ProviderFactory** - Multi-provider abstraction
4. **Capability Registry** - Dynamic hot-reload system
5. **Difficulty Critic** - Safety validation
6. **OpenTelemetry Integration** - Production monitoring

### Data Flow
```
Game State â†’ Snapshot â†’ Hash Check â†’ LLM Provider â†’ Actions â†’ Validation â†’ Execution
```

---

## ðŸš¨ Critical Gap: Capability Implementation

### Promised vs Delivered

**Function Schema** (`planFunction.js`) lists **18 abilities**:
1. dash âœ…
2. radial_burst âœ…
3. cone_aoe âŒ
4. wait âœ…
5. spawn_minions âŒ
6. reposition âŒ
7. taunt âŒ
8. spawn_formation âŒ
9. teleport_to_player âŒ
10. dynamic_movement âŒ
11. charge_attack âŒ
12. pattern_shoot âŒ
13. summon_orbitals âŒ
14. heal_self âŒ
15. shield_phase âŒ
16. effect_aura âŒ
17. conditional_trigger âŒ
18. environment_control âŒ
19. projectile_spread âœ…

**Implemented Capabilities**: **4/18 (22%)**
- âœ… Movement/Dash
- âœ… Emitter/RadialBurst
- âœ… Emitter/ProjectileSpread
- âœ… Core/Wait

**Status:** The LLM can request 14 abilities that **don't exist**, causing plans to silently fail.

---

## ðŸ“Š Google AI Studio API Analysis

### Available Models (Free Tier - 2025)

| Model | RPM | TPM | RPD | Best For |
|-------|-----|-----|-----|----------|
| **Gemini 2.5 Flash** | 10 | 250K | 250 | Fast decisions |
| **Gemini 2.5 Flash-Lite** | 15 | 250K | 1,000 | High-frequency |
| **Gemini 2.5 Pro** | 5 | 250K | 100 | Complex reasoning |

RPM = Requests/minute, TPM = Tokens/minute, RPD = Requests/day

### Key Benefits
- âœ… **Free forever** - Confirmed by Google
- âœ… **1M token context** - Even on free tier
- âœ… **Function calling** - Structured JSON responses
- âœ… **Multiple models** - Switch based on needs

### Current Usage Pattern
- **Frequency:** Every 3 seconds (on state change)
- **Current Load:** ~20 requests/minute (1 game instance)
- **Token Usage:** ~150 tokens/request
- **Daily Usage:** ~28,800 requests/day (continuous play)

**âš ï¸ Problem:** Exceeds free tier limits significantly!

---

## ðŸ”§ Current Implementation Issues

### 1. **Gemini Function Calling Failure**
**Error:** `Gemini: missing functionCall`

**Root Cause:** API returning text response instead of function call

**Impact:** Boss AI non-functional with real API

### 2. **No Request Batching**
**Current:** Individual API call per snapshot (every 3s)
**Problem:**
- Wastes API quota
- High latency per decision
- No bulk planning

### 3. **Single Provider Lock-in**
**Current:** Can only use 1 LLM at a time
**Problem:**
- Can't distribute load across free tiers
- Can't use specialized models for different tasks
- Can't do A/B testing

### 4. **Micro-Request Pattern**
**Current:** Sends tiny snapshots frequently
**Better:** Batch multiple decisions, send context-rich requests less frequently

---

## ðŸŽ¯ Proposed Improvements

### Phase 1: Fix Critical Issues (Priority 1)

#### A. Fix Gemini Function Calling
**Action:** Update GeminiProvider to handle both function calls and text responses

#### B. Implement Missing Core Capabilities
**Implement These 6 High-Impact Abilities:**
1. **cone_aoe** - Directional attack (common pattern)
2. **teleport_to_player** - Positioning (dramatic)
3. **spawn_minions** - Adds complexity
4. **heal_self** - Boss survivability
5. **shield_phase** - Defensive mechanics
6. **charge_attack** - Close-range threat

**Estimated Time:** 2-3 hours (using existing patterns)

---

### Phase 2: Request Optimization (Priority 2)

#### A. Implement Request Batching
**Design:**
```javascript
class BatchedLLMController {
  constructor() {
    this.pendingDecisions = [];
    this.batchWindow = 10; // seconds
    this.maxBatchSize = 5;
  }

  async requestDecision(snapshot) {
    this.pendingDecisions.push(snapshot);

    if (this.shouldFlush()) {
      return this.flushBatch();
    }
  }

  shouldFlush() {
    return (
      this.pendingDecisions.length >= this.maxBatchSize ||
      this.timeSinceLastBatch >= this.batchWindow
    );
  }

  async flushBatch() {
    // Send all snapshots in one prompt
    const batchPrompt = this.buildBatchPrompt(this.pendingDecisions);
    const response = await provider.generate(batchPrompt);
    return this.splitBatchResponses(response);
  }
}
```

**Benefits:**
- Reduce API calls by 80%
- Better token utilization (context sharing)
- More coherent multi-turn planning

---

#### B. Implement Strategic Decision Caching
**Design:**
```javascript
class DecisionCache {
  constructor() {
    this.cache = new Map(); // hash -> decision
    this.ttl = 30000; // 30 seconds
  }

  get(snapshotHash) {
    const entry = this.cache.get(snapshotHash);
    if (entry && Date.now() - entry.timestamp < this.ttl) {
      return entry.decision;
    }
    return null;
  }
}
```

**When to Cache:**
- Similar game states (health bands, player distances)
- Early game / late game phases
- Common tactical scenarios

---

### Phase 3: Multi-LLM Architecture (Priority 3)

#### A. Provider Pool System
**Design:**
```javascript
class LLMProviderPool {
  constructor() {
    this.providers = [
      new GeminiProvider('key1', { model: 'flash' }),      // Fast decisions
      new GeminiProvider('key2', { model: 'flash-lite' }), // High frequency
      new GeminiProvider('key3', { model: 'pro' }),        // Complex plans
      new OllamaProvider({ model: 'llama3' }),             // Fallback
      new OpenAIProvider('key4', { model: 'gpt-4o-mini' }) // Alternative
    ];
    this.requestCounts = new Map();
  }

  async route(snapshot, priority) {
    // Use Flash-Lite for simple decisions
    if (priority === 'low') return this.providers[1];

    // Use Pro for boss phase transitions
    if (snapshot.boss.phase !== snapshot.lastPhase) return this.providers[2];

    // Round-robin on Flash for normal
    return this.getNextAvailable();
  }

  getNextAvailable() {
    // Find provider with lowest request count
    return this.providers.reduce((min, provider) => {
      const count = this.requestCounts.get(provider) || 0;
      const minCount = this.requestCounts.get(min) || 0;
      return count < minCount ? provider : min;
    });
  }
}
```

**Benefits:**
- **5x more requests/day** (using 5 free API keys)
- Smart model selection (fast vs smart)
- Automatic failover
- Cost optimization

---

#### B. Add OpenAI Support
**New Providers to Add:**
```
âœ… Gemini (Google AI Studio) - Current
ðŸ”² GPT-4o-mini (OpenAI) - Fast, cheap
ðŸ”² Claude 3.5 Haiku (Anthropic) - Quality
ðŸ”² Ollama (Local) - Free, offline
ðŸ”² Groq (Fast inference) - Low latency
```

---

### Phase 4: Advanced Features (Priority 4)

#### A. Adaptive Request Frequency
**Current:** Fixed 3-second interval
**Better:** Dynamic based on game state

```javascript
calculateNextCallDelay(gameState) {
  // More frequent during combat
  if (gameState.activePlayers > 2) return 2.0;

  // Less frequent when no players
  if (gameState.activePlayers === 0) return 30.0;

  // Urgent during boss phase changes
  if (gameState.boss.healthPercent < 0.25) return 1.0;

  return 3.0; // default
}
```

---

#### B. Progressive Decision Making
**Concept:** Make quick decisions with cheap model, refine with expensive model

```javascript
async decideTwoStage(snapshot) {
  // Stage 1: Fast triage (Flash-Lite, 0.3s)
  const quickDecision = await flashLite.generate(snapshot);

  // If decision is "complex", elevate to Pro
  if (quickDecision.complexity === 'high') {
    return await proPovider.generate(snapshot);
  }

  return quickDecision;
}
```

---

## ðŸ“ˆ Expected Impact

### Current State (Baseline)
- **Requests/Day:** 28,800 (exceeds all free tiers)
- **Capabilities:** 4/18 (22%)
- **Failure Rate:** High (function call issues)
- **Cost:** $0 (not working)

### After Phase 1 (Critical Fixes)
- **Requests/Day:** 28,800
- **Capabilities:** 10/18 (56%)
- **Failure Rate:** Low
- **Cost:** $0 (works within 1 free tier if played <30 min/day)

### After Phase 2 (Batching)
- **Requests/Day:** 5,760 (80% reduction)
- **Capabilities:** 10/18 (56%)
- **Failure Rate:** Very Low
- **Cost:** $0 (works within free tier)

### After Phase 3 (Multi-LLM)
- **Requests/Day:** 28,800 (distributed across 5 APIs)
- **Capabilities:** 10/18 (56%)
- **Failure Rate:** Minimal (fallback support)
- **Cost:** $0 (5x free tier capacity)
- **Uptime:** 99.9% (redundancy)

---

## ðŸ”¨ Implementation Roadmap

### Week 1: Core Functionality
- [x] API key setup and testing
- [ ] Fix Gemini function calling
- [ ] Implement 6 missing core capabilities
- [ ] Test boss behavior with real API

### Week 2: Optimization
- [ ] Implement request batching
- [ ] Add decision caching
- [ ] Implement adaptive frequency
- [ ] Load test and tune

### Week 3: Multi-LLM
- [ ] Add OpenAI provider
- [ ] Add Anthropic provider
- [ ] Implement provider pool
- [ ] Smart routing logic

### Week 4: Polish
- [ ] Progressive decision making
- [ ] Enhanced telemetry
- [ ] A/B testing framework
- [ ] Performance optimization

---

## ðŸ’° Cost Analysis

### Scenario: 24/7 Server (1 Instance)

| Configuration | Requests/Day | Cost/Day | Cost/Month |
|---------------|--------------|----------|------------|
| Current (broken) | 28,800 | $0 | $0 |
| Phase 1 (fixed) | 28,800 | $8.64* | $259 |
| Phase 2 (batched) | 5,760 | $0 | $0 |
| Phase 3 (multi-LLM) | 28,800 | $0 | $0 |

*Assumes paid tier if exceeded

**Recommendation:** Implement Phase 2 ASAP to stay in free tier

---

## ðŸŽ® Capability Implementation Priority

### Tier 1: Must-Have (Week 1)
1. **cone_aoe** - Bread-and-butter attack
2. **teleport_to_player** - Mobility
3. **spawn_minions** - Complexity

### Tier 2: Should-Have (Week 2)
4. **heal_self** - Survivability
5. **shield_phase** - Defense
6. **charge_attack** - Variety

### Tier 3: Nice-to-Have (Week 3)
7. **summon_orbitals** - Visual appeal
8. **effect_aura** - Area control
9. **pattern_shoot** - Advanced patterns

### Tier 4: Advanced (Week 4)
10. **conditional_trigger** - Reactive AI
11. **environment_control** - Map interaction
12. **dynamic_movement** - Advanced positioning

---

## ðŸ”’ Safety Considerations

### Rate Limiting
- Keep per-model limits under free tier
- Implement exponential backoff on 429 errors
- Queue overflow handling

### Cost Protection
```javascript
class CostGuard {
  constructor(dailyLimit = 1000) {
    this.dailyLimit = dailyLimit;
    this.requestsToday = 0;
    this.lastReset = Date.now();
  }

  async checkBeforeCall() {
    this.resetIfNewDay();

    if (this.requestsToday >= this.dailyLimit) {
      throw new Error('Daily API limit reached');
    }

    this.requestsToday++;
  }
}
```

---

## ðŸ“š Next Steps

1. **IMMEDIATE:** Fix Gemini function calling issue
2. **TODAY:** Implement 3 missing capabilities (cone_aoe, teleport, spawn_minions)
3. **THIS WEEK:** Add request batching
4. **NEXT WEEK:** Multi-LLM support

---

## ðŸ¤” Open Questions

1. Should we implement plan caching? (Recommended: Yes, 30s TTL)
2. Which LLM providers to prioritize? (Recommended: OpenAI GPT-4o-mini next)
3. Should we support streaming responses? (Recommended: Phase 4)
4. Do we need plan versioning for rollback? (Recommended: Not yet)

---

**Generated by:** Claude Code Analysis
**Last Updated:** October 24, 2025
